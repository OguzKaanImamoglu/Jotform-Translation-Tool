{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nmt_with_attention.ipnyb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_qNSzzyaCbD"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jmjh290raIky"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Neural machine translation with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOpGoE2T-YXS"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/nmt_with_attention\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/nmt_with_attention.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/nmt_with_attention.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/nmt_with_attention.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAmSR1FaqKrl"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGFTkuRvzWqc",
        "outputId": "7cee9419-d134-4364-b64e-c699f2510c6a"
      },
      "source": [
        "!pip install tensorflow_text"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: tensorflow<2.6,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.34.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.36.2)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (3.12.4)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.31.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.4.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (57.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.3.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (4.5.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import tensorflow_text as tf_text\n",
        "from numpy import array, argmax, random, take\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs8zge-RUdC2"
      },
      "source": [
        "This tutorial builds a few layers from scratch, use this variable if you want to switch between the custom and builtin implementations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPJ9J7iPUchc"
      },
      "source": [
        "use_builtins = True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_yq8kvIqoqQ"
      },
      "source": [
        "This tutorial uses a lot of low level API's where it's easy to get shapes wrong. This class is used to check shapes throughout the tutorial. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqFqKi4fqN9X"
      },
      "source": [
        "#@title Shape checker\n",
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    if isinstance(names, str):\n",
        "      names = (names,)\n",
        "\n",
        "    shape = tf.shape(tensor)\n",
        "    rank = tf.rank(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'    found {rank}: {shape.numpy()}\\n'\n",
        "                       f'    expected {len(names)}: {names}\\n')\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dim = name\n",
        "      else:\n",
        "        old_dim = self.shapes.get(name, None)\n",
        "      new_dim = shape[i]\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjUROhJfH3ML"
      },
      "source": [
        "## The data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvlhdlnvqDPm",
        "outputId": "7055b5af-c1b7-46d7-e498-54b75cb066cb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMO8iqEqqQOP"
      },
      "source": [
        "path ='/content/drive/My Drive/Translation/Datasets/Final/entr.txt'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzQOFqQCrPW3"
      },
      "source": [
        "# function to read raw text file\n",
        "def read_text(filename):\n",
        "    # open the file\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbPY8_Csrq7w"
      },
      "source": [
        "# split a text into sentences\n",
        "def to_lines(text):\n",
        "    sents = text.strip().split('\\n')\n",
        "    sents = [i.split('\\t') for i in sents]\n",
        "    return sents"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs74pAm0rW9Q",
        "outputId": "8705af45-9599-46d4-8a04-ec3e17aa5470"
      },
      "source": [
        "data = read_text(path)\n",
        "deu_eng = to_lines(data)\n",
        "deu_eng = array(deu_eng)\n",
        "print(deu_eng.shape)\n",
        "dataset = deu_eng[:100000]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(519142, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWVkcSMtr5Nz",
        "outputId": "20033690-cefd-444a-8e01-3f4f03f77075"
      },
      "source": [
        "print(dataset.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVAKa6YVsfku"
      },
      "source": [
        "inp = dataset[:,1]\n",
        "inp = inp.tolist()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6a5qoNbsBUO"
      },
      "source": [
        "targ = dataset[:,0]\n",
        "targ = targ.tolist()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc1ciQxnsK6W",
        "outputId": "28ecf26b-a0c3-4ff0-872d-7b1b6077bff1"
      },
      "source": [
        "print(len(inp))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfVWx3WaI5Df"
      },
      "source": [
        "From these arrays of strings you can create a `tf.data.Dataset` of strings that shuffles and batches them efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqHsArVZ3jFS"
      },
      "source": [
        "BUFFER_SIZE = len(inp)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc6-NK1GtWQt",
        "outputId": "cd6fadcf-abdc-4863-c709-0010e410b0ba"
      },
      "source": [
        "for example_input_batch, example_target_batch in dataset.take(1):\n",
        "  print(example_input_batch[:5])\n",
        "  print()\n",
        "  print(example_target_batch[:5])\n",
        "  break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'Yapt\\xc4\\xb1\\xc4\\x9f\\xc4\\xb1m\\xc4\\xb1z o de\\xc4\\x9fil.'\n",
            " b\"Sadece Tom'un gitmesine izin ver.\" b'Tom orada de\\xc4\\x9fildi.'\n",
            " b'Hi\\xc3\\xa7bir \\xc5\\x9fey olmayacak.'\n",
            " b'Siz sigara i\\xc3\\xa7iyor musunuz?'], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b\"That's not what we do.\" b'Just let Tom go.' b\"Tom wasn't there.\"\n",
            " b\"Nothing'll happen.\" b'Do you guys smoke?'], shape=(5,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCoxLcuN3bwv"
      },
      "source": [
        "### Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kwdPcHvzz_a"
      },
      "source": [
        "One of the goals of this tutorial is to build a model that can be exported as a `tf.saved_model`. To make that exported model useful it should take `tf.string` inputs, and retrun `tf.string` outputs: All the text processing happens inside the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOQ5n55X4uDB"
      },
      "source": [
        "#### Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upKhKAMK4zzI"
      },
      "source": [
        "The model is dealing with multilingual text with a limited vocabulary. So it will be important to standardize the input text.\n",
        "\n",
        "The first step is Unicode normalization to split accented characters and replace compatibility characters with their ASCII equivalents.\n",
        "\n",
        "The `tensroflow_text` package contains a unicode normalize operation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD0e-DWGQ2Vo",
        "outputId": "92ac10cd-d3aa-4221-8045-8669b9ef84a1"
      },
      "source": [
        "example_text = tf.constant('¿Hasta olduğunuz doğru mu?')\n",
        "\n",
        "print(example_text.numpy())\n",
        "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'\\xc2\\xbfHasta oldu\\xc4\\x9funuz do\\xc4\\x9fru mu?'\n",
            "b'\\xc2\\xbfHasta oldug\\xcc\\x86unuz dog\\xcc\\x86ru mu?'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hTllEjK6RSo"
      },
      "source": [
        "Unicode normalization will be the first step in the text standardization function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chTF5N885F0P"
      },
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accecented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UREvDg3sEKYa",
        "outputId": "3403b4dd-b241-4fe7-b012-93983c11d5af"
      },
      "source": [
        "print(example_text.numpy().decode())\n",
        "print(tf_lower_and_split_punct(example_text).numpy().decode())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "¿Hasta olduğunuz doğru mu?\n",
            "[START] ¿ hasta oldugunuz dogru mu ? [END]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q-sKsSI7xRZ"
      },
      "source": [
        "#### Text Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aKn8qd37abi"
      },
      "source": [
        "This standardization function will be wrapped up in a `preprocessing.TextVectorization` layer which will handle the vocabulary extraction and conversion of input text to sequences of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "source": [
        "max_vocab_size = 10000\n",
        "\n",
        "input_text_processor = preprocessing.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kbC6ODP8IK_"
      },
      "source": [
        "The `TextVectorization` layer and many other `experimental.preprocessing` layers have an `adapt` method. This method reads one epoch of the training data, and works a lot like `Model.fix`. This `adapt` method initializes the layer based on the data. Here it determines the vocabulary:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmsI1Yql8FYe",
        "outputId": "7feeccd0-f99d-4620-831c-021cd763a25e"
      },
      "source": [
        "input_text_processor.adapt(inp)\n",
        "\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "input_text_processor.get_vocabulary()[:10]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'tom', '?', 'bir', 'o', 'ben']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlC4xuZnKLBS",
        "outputId": "538f79aa-fe55-488e-f6e5-c0cbd0ffb9ee"
      },
      "source": [
        "output_text_processor = preprocessing.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor.adapt(targ)\n",
        "output_text_processor.get_vocabulary()[:10]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'tom', 'i', '?', 'you', 'is']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQqlP_s9eIv"
      },
      "source": [
        "Now these layers can convert a batch of strings into a batch of token IDs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KZxj8IrNZ9S",
        "outputId": "224ffb51-3613-41f4-eac1-c5761c5f558e"
      },
      "source": [
        "example_tokens = input_text_processor(example_input_batch)\n",
        "example_tokens[:3, :10]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 8), dtype=int64, numpy=\n",
              "array([[   2, 2253,    8,   17,    4,    3,    0,    0],\n",
              "       [   2,   32,   15,  905,   82,   75,    4,    3],\n",
              "       [   2,    5,   88,   83,    4,    3,    0,    0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA9rUn9G9n78"
      },
      "source": [
        "The `get_vocabulary` method can be used to convert token IDs back to text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "98g9rcxGQY0I",
        "outputId": "5e3155d4-6439-4889-8f82-ae273f8b4396"
      },
      "source": [
        "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
        "tokens = input_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[START] yaptgmz o degil . [END]  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot0aCL9t-Ghi"
      },
      "source": [
        "The returned token IDs are zero-padded. This can easily be turned into a mask:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "_jx4Or_eFRSz",
        "outputId": "a65873df-4813-4325-cedf-7cac4ac3a75c"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(example_tokens)\n",
        "plt.title('Token IDs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNfHIF71ulLu"
      },
      "source": [
        "## The encoder/decoder model\n",
        "\n",
        "The following diagram shows an overview of the model. At each time-step the decoder's output is combined with a weighted sum over the encoded input, to predict the next word. The diagram and formulas are from [Luong's paper](https://arxiv.org/abs/1508.04025v5).\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzQWx2saImMV"
      },
      "source": [
        "Before getting into it define a few constants for the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a9uNz3-IrF-"
      },
      "source": [
        "embedding_dim = 256\n",
        "units = 1024"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blNgVbLSzpsr"
      },
      "source": [
        "### The encoder\n",
        "\n",
        "Start by building the encoder, the blue part of the diagram above.\n",
        "\n",
        "The encoder:\n",
        "\n",
        "1. Takes a list of token IDs (from `input_text_processor`).\n",
        "3. Looks up an embedding vector for each token (Using a `layers.Embedding`).\n",
        "4. Processes the embeddings into a new sequence (Using a `layers.GRU`).\n",
        "5. Returns:\n",
        "  * The processed sequence. This will be passed to the attention head.\n",
        "  * The internal state. This will be used to initialize the decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(tokens, ('batch', 's'))\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding for each token.\n",
        "    vectors = self.embedding(tokens)\n",
        "    shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence.\n",
        "    #    output shape: (batch, s, enc_units)\n",
        "    #    state shape: (batch, enc_units)\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "    shape_checker(output, ('batch', 's', 'enc_units'))\n",
        "    shape_checker(state, ('batch', 'enc_units'))\n",
        "\n",
        "    # 4. Returns the new sequence and its state.\n",
        "    return output, state"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3SKkaQeGn-Q"
      },
      "source": [
        "Here is how it fits together so far:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60gSVh05Jl6l",
        "outputId": "db0bb63a-8288-4bdf-d8cc-f086e487f297"
      },
      "source": [
        "# Convert the input text to tokens.\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "\n",
        "# Encode the input sequence.\n",
        "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)\n",
        "example_enc_output, example_enc_state = encoder(example_tokens)\n",
        "\n",
        "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
        "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
        "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input batch, shape (batch): (64,)\n",
            "Input batch tokens, shape (batch, s): (64, 8)\n",
            "Encoder output, shape (batch, s, units): (64, 8, 1024)\n",
            "Encoder state, shape (batch, units): (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RIPHh4O9ixB"
      },
      "source": [
        "The encoder returns its internal state so that its state can be used to initialize the decoder.\n",
        "\n",
        "It's also common for an RNN to return its state so that it can process a sequence over multiple calls. You'll see more of that building the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45xM_Gl1MgXY"
      },
      "source": [
        "### The attention head\n",
        "\n",
        "The decoder uses attention to selectively focus on parts of the input sequence.\n",
        "The attention takes a sequence of vectors as input for each example and returns an \"attention\" vector for each example. This attention layer is similar to a `layers.GlobalAveragePoling1D` but the attention layer performs a _weighted_ average.\n",
        "\n",
        "Let's look at how this works:\n",
        "\n",
        "<img src=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/images/attention_equation_1.jpg?raw=1\" alt=\"attention equation 1\" width=\"800\">\n",
        "\n",
        "<img src=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/images/attention_equation_2.jpg?raw=1\" alt=\"attention equation 2\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX2JsKzzzgZ5"
      },
      "source": [
        "Where:\n",
        "\n",
        "* $s$ is the encoder index.\n",
        "* $t$ is the decoder index.\n",
        "* $\\alpha_{ts}$ is the attention weights.\n",
        "* $h_s$ is the sequence of encoder outputs being attended to (the attention \"key\" and \"value\" in transformer terminology).\n",
        "* $h_t$ is the the decoder state attending to the sequence (the attention \"query\" in transformer terminology).\n",
        "* $c_t$ is the resulting context vector.\n",
        "* $a_t$ is the final output combining the \"context\" and \"query\".\n",
        "\n",
        "The equations:\n",
        "\n",
        "1. Calculates the attention weights, $\\alpha_{ts}$, as a softmax across the encoder's output sequence.\n",
        "2. Calculates the context vector as the weighted sum of the encoder outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNA5GeHHPsGL"
      },
      "source": [
        "Last is the $score$ function. Its job is to calculate a scalar logit-score for each key-query pair. There are two common approaches:\n",
        "\n",
        "<img src=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/images/attention_equation_4.jpg?raw=1\" alt=\"attention equation 4\" width=\"800\">\n",
        "\n",
        "This tutorial uses [Bahdanau's additive attention](https://arxiv.org/pdf/1409.0473.pdf). TensorFlow includes implementations of both as `layers.Attention` and\n",
        "`layers.AdditiveAttention`. The class below handles the weight matrices in a pair of `layers.Dense` layers, and calls the builtin implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "momiE59lXo6U"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # For Eqn. (4), the  Bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(query, ('batch', 't', 'query_units'))\n",
        "    shape_checker(value, ('batch', 's', 'value_units'))\n",
        "    shape_checker(mask, ('batch', 's'))\n",
        "\n",
        "    # From Eqn. (4), `W1@ht`.\n",
        "    w1_query = self.W1(query)\n",
        "    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
        "\n",
        "    # From Eqn. (4), `W2@hs`.\n",
        "    w2_key = self.W2(value)\n",
        "    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
        "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf13LubPGjDO"
      },
      "source": [
        "### Test the Attention layer\n",
        "\n",
        "Create a `BahdanauAttention` layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4QMlOp8Gidh"
      },
      "source": [
        "attention_layer = BahdanauAttention(units)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snA1uL9AI-JE"
      },
      "source": [
        "This layer takes 3 inputs:\n",
        "\n",
        "* The `query`: This will be generated by the decoder, later.\n",
        "* The `value`: This Will be the output of the encoder.\n",
        "* The `mask`: To exclude the padding, `example_tokens != 0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYSHqmORgVFo",
        "outputId": "84c64647-5897-42ef-f089-e4bca0f97c64"
      },
      "source": [
        "(example_tokens != 0).shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2bmvT25pXnr"
      },
      "source": [
        "The vectorized implementation of the attention layer lets you pass a batch of sequences of query vectors and a batch of sequence of value vectors. The result is:\n",
        "\n",
        "1. A batch of sequences of result vectors the size of the queries.\n",
        "2. A batch attention maps, with size `(query_length, value_length)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y7hjPkNMmHh",
        "outputId": "3aaff263-f292-45c3-c26a-87fd0ab8015b"
      },
      "source": [
        "# Later, the decoder will generate this attention query\n",
        "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
        "\n",
        "# Attend to the encoded tokens\n",
        "\n",
        "context_vector, attention_weights = attention_layer(\n",
        "    query=example_attention_query,\n",
        "    value=example_enc_output,\n",
        "    mask=(example_tokens != 0))\n",
        "\n",
        "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
        "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch_size, query_seq_length, units):           (64, 2, 1024)\n",
            "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (64, 2, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AagyXMH-Jhqt"
      },
      "source": [
        "The attention weights should sum to `1.0` for each sequence.\n",
        "\n",
        "Here are the attention weights across the sequences at `t=0`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "Rqr8XGsAJlf6",
        "outputId": "4c19b9d6-ecde-4609-95d3-75fd0b005d1a"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(attention_weights[:, 0, :])\n",
        "plt.title('Attention weights')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Eil-C_NN1rp"
      },
      "source": [
        "Because of the small-random initialization the attention weights are all close to `1/(sequence_length)`. If you zoom in on the weights for a single sequence, you can see that there is some _small_ variation that the model can learn to expand, and exploit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuzrCdmYlTcJ",
        "outputId": "4e7251c0-ba97-4586-823a-9c559e7b3997"
      },
      "source": [
        "attention_weights.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 2, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIMwC-f-ZC8N"
      },
      "source": [
        "attention_slice = attention_weights[0, 0].numpy()\n",
        "attention_slice = attention_slice[attention_slice != 0]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "ysWDPO6hOS8X",
        "outputId": "8501b647-706f-41f9-f732-cf68eade7d18"
      },
      "source": [
        "#@title\n",
        "plt.suptitle('Attention weights for one sequence')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "a1 = plt.subplot(1, 2, 1)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "# freeze the xlim\n",
        "plt.xlim(plt.xlim())\n",
        "plt.xlabel('Attention weights')\n",
        "\n",
        "a2 = plt.subplot(1, 2, 2)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "plt.xlabel('Attention weights, zoomed')\n",
        "\n",
        "# zoom in\n",
        "top = max(a1.get_ylim())\n",
        "zoom = 0.85*top\n",
        "a2.set_ylim([0.90*top, top])\n",
        "a1.plot(a1.get_xlim(), [zoom, zoom], color='k')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3bf44cb5d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ638eHN4iCK"
      },
      "source": [
        "### The decoder\n",
        "\n",
        "The decoder's job is to generate predictions for the next output token.\n",
        "\n",
        "1. The decoder receives the complete encoder output.\n",
        "2. It uses an RNN to keep track of what it has generated so far.\n",
        "3. It uses its RNN output as the query to the attention over the encoder's output, producing the context vector.\n",
        "4. It combines the RNN output and the context vector using Equation 3 (below) to generate the \"attention vector\".\n",
        "5. It generates logit predictions for the next token based on the \"attention vector\".\n",
        "\n",
        "<img src=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/images/attention_equation_3.jpg?raw=1\" alt=\"attention equation 3\" width=\"800\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZsQJMqNmg_L"
      },
      "source": [
        "Here is the `Decoder` class and its initializer. The initializer creates all the necessary layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erYvHIgAl8kh"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # For Step 1. The embedding layer convets token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # For step 3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
        "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "\n",
        "    # For step 5. This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUTfYHmfmwKH"
      },
      "source": [
        "The `call` method for this layer takes and returns multiple tensors. Organize those into simple container classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WfSIb2sArRT"
      },
      "source": [
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NChkl2KrnV2y"
      },
      "source": [
        "Here is the implementation of the `call` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJOi5btHAPNK"
      },
      "source": [
        "def call(self,\n",
        "         inputs: DecoderInput,\n",
        "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(inputs.new_tokens, ('batch', 't'))\n",
        "  shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
        "  shape_checker(inputs.mask, ('batch', 's'))\n",
        "\n",
        "  if state is not None:\n",
        "    shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "  # Step 1. Lookup the embeddings\n",
        "  vectors = self.embedding(inputs.new_tokens)\n",
        "  shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
        "\n",
        "  # Step 2. Process one step with the RNN\n",
        "  rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "  shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
        "  shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "  # Step 3. Use the RNN output as the query for the attention over the\n",
        "  # encoder output.\n",
        "  context_vector, attention_weights = self.attention(\n",
        "      query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "  shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
        "  shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "  # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
        "  #     [ct; ht] shape: (batch t, value_units + query_units)\n",
        "  context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
        "  attention_vector = self.Wc(context_and_rnn_output)\n",
        "  shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
        "\n",
        "  # Step 5. Generate logit predictions:\n",
        "  logits = self.fc(attention_vector)\n",
        "  shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
        "\n",
        "  return DecoderOutput(logits, attention_weights), state"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay_mTMPfnb2a"
      },
      "source": [
        "Decoder.call = call"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arTOBklcFTiC"
      },
      "source": [
        "The **encoder** processes its full input sequence with a single call to its RNN. This implementation of the **decoder** _can_ do that as well for efficient training. But this tutorial will run the decoder in a loop for a few reasons:\n",
        "\n",
        "* Flexibility: Writing the loop gives you direct control over the training procedure.\n",
        "* Clarity: It's possible to do masking tricks and use `layers.RNN`, or `tfa.seq2seq` APIs to pack this all into a single call. But writing it out as a loop may be clearer. \n",
        "  * Loop free training is demonstrated in the [Text generation](text_generation.ipynb) tutiorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1-mLAcUEXpK"
      },
      "source": [
        "Now try using this decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZUMbYXIEVeA"
      },
      "source": [
        "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPnaw583CpnY"
      },
      "source": [
        "The decoder takes 4 inputs.\n",
        "\n",
        "* `new_tokens` -  The last token generated. Initialize the decoder with the `\"[START]\"` token.\n",
        "* `enc_output` - Generated by the `Encoder`.\n",
        "* `mask` - A boolean tensor indicating where `tokens != 0`\n",
        "* `state` - The previous `state` output from the decoder (the internal state\n",
        "  of the decoder's RNN). Pass `None` to zero-initialize it. The original\n",
        "  paper initializes it from the encoder's final RNN state. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u6eJBU4GL40"
      },
      "source": [
        "# Convert the target sequence, and collect the \"[START]\" tokens\n",
        "example_output_tokens = output_text_processor(example_target_batch)\n",
        "\n",
        "start_index = output_text_processor._index_lookup_layer('[START]').numpy()\n",
        "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5hqvbR5FUCD",
        "outputId": "5a9d3c2a-135e-485d-e67d-f1e66a935d74"
      },
      "source": [
        "# Run the decoder\n",
        "dec_result, dec_state = decoder(\n",
        "    inputs = DecoderInput(new_tokens=first_token,\n",
        "                          enc_output=example_enc_output,\n",
        "                          mask=(example_tokens != 0)),\n",
        "    state = example_enc_state\n",
        ")\n",
        "\n",
        "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
        "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logits shape: (batch_size, t, output_vocab_size) (64, 1, 10000)\n",
            "state shape: (batch_size, dec_units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEZvXZRVPHd6"
      },
      "source": [
        "Sample a token according to the logits:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5UY8wko3jFp"
      },
      "source": [
        "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xTpX44VkzrY"
      },
      "source": [
        "Decode the token as the first word of the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKXTLYu4IV7I",
        "outputId": "6b0913f1-e540-4eac-8a0e-3705ec20c336"
      },
      "source": [
        "vocab = np.array(output_text_processor.get_vocabulary())\n",
        "first_word = vocab[sampled_token.numpy()]\n",
        "first_word[:5]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['creaked'],\n",
              "       ['millionaire'],\n",
              "       ['jammed'],\n",
              "       ['pushover'],\n",
              "       ['fresh']], dtype='<U23')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUQV6AXoQR7z"
      },
      "source": [
        "Now use the decoder to generate a second set of logits.\n",
        "\n",
        "- Pass the same `enc_output` and `mask`, these haven't changed.\n",
        "- Pass the sampled token as `new_tokens`.\n",
        "- Pass the `decoder_state` the decoder returned last time, so the RNN continues with a memory of where it left off last time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX1VF9XDJTOM"
      },
      "source": [
        "dec_result, dec_state = decoder(\n",
        "    DecoderInput(sampled_token,\n",
        "                 example_enc_output,\n",
        "                 mask=(example_tokens != 0)),\n",
        "    state=dec_state)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1rs0XL7Y2aS",
        "outputId": "2a0402ee-309f-47c6-e14c-0b0d201a413a"
      },
      "source": [
        "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
        "first_word = vocab[sampled_token.numpy()]\n",
        "first_word[:5]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['discuss'],\n",
              "       ['representative'],\n",
              "       ['scammed'],\n",
              "       ['release'],\n",
              "       ['repulsive']], dtype='<U23')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6xyru86m914"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now that you have all the model components, it's time to start training the model. You'll need:\n",
        "\n",
        "- A loss function and optimizer to perform the optimization.\n",
        "- A training step function defining how to update the model for each input/target batch.\n",
        "- A training loop to drive the training and save checkpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "### Define the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(y_true, ('batch', 't'))\n",
        "    shape_checker(y_pred, ('batch', 't', 'logits'))\n",
        "\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    shape_checker(loss, ('batch', 't'))\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    shape_checker(mask, ('batch', 't'))\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5AgEBh2S404"
      },
      "source": [
        "### Implement the training step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_G20Te1XSmJ"
      },
      "source": [
        "Start with a model class, the training process will be implemented as the `train_step` method on this model. See [Customizing fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit) for details.\n",
        "\n",
        "Here the `train_step` method is a wrapper around the `_train_step` implementation which will come later. This wrapper includes a switch to turn on and off `tf.function` compilation, to make debugging easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWIyuy71TkJT"
      },
      "source": [
        "class TrainTranslator(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               input_text_processor,\n",
        "               output_text_processor, \n",
        "               use_tf_function=True):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "    self.use_tf_function = use_tf_function\n",
        "    self.shape_checker = ShapeChecker()\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    self.shape_checker = ShapeChecker()\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i0i1x6jwsLm"
      },
      "source": [
        "Overall the implementation for the `Model.train_step` method is as follows:\n",
        "\n",
        "1. Receive a batch of `input_text, target_text` from the `tf.data.Dataset`.\n",
        "2. Convert those raw text inputs to token-embeddings and masks. \n",
        "3. Run the encoder on the `input_tokens` to get the `encoder_output` and `encoder_state`.\n",
        "4. Initialize the decoder state and loss. \n",
        "5. Loop over the `target_tokens`:\n",
        "   1. Run the decoder one step at a time.\n",
        "   2. Calculate the loss for each step.\n",
        "   3. Accumulate the average loss.\n",
        "6. Calculate the gradient of the loss and use the optimizer to apply updates to the model's `trainable_variables`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngBjFw4BU5G7"
      },
      "source": [
        "The `_preprocess` method, added below, implements steps #1 and #2: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlYE68wzXoA8"
      },
      "source": [
        "def _preprocess(self, input_text, target_text):\n",
        "  self.shape_checker(input_text, ('batch',))\n",
        "  self.shape_checker(target_text, ('batch',))\n",
        "\n",
        "  # Convert the text to token IDs\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  target_tokens = self.output_text_processor(target_text)\n",
        "  self.shape_checker(input_tokens, ('batch', 's'))\n",
        "  self.shape_checker(target_tokens, ('batch', 't'))\n",
        "\n",
        "  # Convert IDs to masks.\n",
        "  input_mask = input_tokens != 0\n",
        "  self.shape_checker(input_mask, ('batch', 's'))\n",
        "\n",
        "  target_mask = target_tokens != 0\n",
        "  self.shape_checker(target_mask, ('batch', 't'))\n",
        "\n",
        "  return input_tokens, input_mask, target_tokens, target_mask"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHy6hzStrgjQ"
      },
      "source": [
        "TrainTranslator._preprocess = _preprocess"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3kvbcArc2y-"
      },
      "source": [
        "The `_train_step` method, added below, handles the remaining steps except for actually running the decoder: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs_gsISsYPpY"
      },
      "source": [
        "def _train_step(self, inputs):\n",
        "  input_text, target_text = inputs  \n",
        "\n",
        "  (input_tokens, input_mask,\n",
        "   target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "  max_target_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Encode the input\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "    self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
        "    self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
        "\n",
        "    # Initialize the decoder's state to the encoder's final state.\n",
        "    # This only works if the encoder and decoder have the same number of\n",
        "    # units.\n",
        "    dec_state = enc_state\n",
        "    loss = tf.constant(0.0)\n",
        "\n",
        "    for t in tf.range(max_target_length-1):\n",
        "      # Pass in two tokens from the target sequence:\n",
        "      # 1. The current input to the decoder.\n",
        "      # 2. The target the target for the decoder's next prediction.\n",
        "      new_tokens = target_tokens[:, t:t+2]\n",
        "      step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                             enc_output, dec_state)\n",
        "      loss = loss + step_loss\n",
        "\n",
        "    # Average the loss over all non padding tokens.\n",
        "    average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "  # Apply an optimization step\n",
        "  variables = self.trainable_variables \n",
        "  gradients = tape.gradient(average_loss, variables)\n",
        "  self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  # Return a dict mapping metric names to current value\n",
        "  return {'batch_loss': average_loss}"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGwWHIxLrjGR"
      },
      "source": [
        "TrainTranslator._train_step = _train_step"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7g40o-mXyt5"
      },
      "source": [
        "The `_loop_step` method, added below, executes the decoder and calculates the incremental loss and new decoder state (`dec_state`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VrzgwztXzYJ"
      },
      "source": [
        "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "  input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "  # Run the decoder one step.\n",
        "  decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                               enc_output=enc_output,\n",
        "                               mask=input_mask)\n",
        "\n",
        "  dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "  self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
        "  self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
        "  self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
        "\n",
        "  # `self.loss` returns the total for non-padded tokens\n",
        "  y = target_token\n",
        "  y_pred = dec_result.logits\n",
        "  step_loss = self.loss(y, y_pred)\n",
        "\n",
        "  return step_loss, dec_state"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj3I7VULrk1R"
      },
      "source": [
        "TrainTranslator._loop_step = _loop_step"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WACCHvKWBQ9C"
      },
      "source": [
        "### Test the training step\n",
        "\n",
        "Build a `TrainTranslator`, and configure it for training using the `Model.compile` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA6bCske8TXm"
      },
      "source": [
        "translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "    use_tf_function=False)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y5OnZDsB3sB"
      },
      "source": [
        "Test out the `train_step`. For a text model like this the loss should start near:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHe-OudqCFGK",
        "outputId": "2c564bf3-4d4f-49db-ee64-79e1c56a464f"
      },
      "source": [
        "np.log(output_text_processor.vocabulary_size())"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.210340371976184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwMU9cFEfjha",
        "outputId": "91b602ca-5f01-494f-f339-b58844141e5a"
      },
      "source": [
        "%%time\n",
        "for n in range(10):\n",
        "  print(translator.train_step([example_input_batch, example_target_batch]))\n",
        "print()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.82057>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.790251>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.7330256>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.580264>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.123758>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.6542225>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.2971854>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.8897736>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.533429>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.392075>}\n",
            "\n",
            "CPU times: user 3.32 s, sys: 241 ms, total: 3.56 s\n",
            "Wall time: 3.19 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-xqtsMbCUp2"
      },
      "source": [
        "While it's easier to debug without a `tf.function` it does give a performance boost. So now that the `_train_step` method is working, try the `tf.function`-wrapped `_tf_train_step`, to maximize performance while training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFUsTKQx0jaH"
      },
      "source": [
        "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "def _tf_train_step(self, inputs):\n",
        "  return self._train_step(inputs)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-bgU59jrztQ"
      },
      "source": [
        "TrainTranslator._tf_train_step = _tf_train_step"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC8bRv_Gr3H9"
      },
      "source": [
        "translator.use_tf_function = True"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKMYNF_sIFb9"
      },
      "source": [
        "The first call will be slow, because it traces the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLQZsX2dp1QK",
        "outputId": "987ad661-cb96-40e9-ce35-c30df604eca6"
      },
      "source": [
        "translator.train_step([example_input_batch, example_target_batch])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.4450452>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3t2Hg7UISYi"
      },
      "source": [
        "But after that it's usually 2-3x faster than the eager `train_step` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzXXMwjXCqqh",
        "outputId": "ffad2496-3c4d-406f-8038-a18c839f7eb2"
      },
      "source": [
        "%%time\n",
        "for n in range(10):\n",
        "  print(translator.train_step([example_input_batch, example_target_batch]))\n",
        "print()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.4376833>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.4049582>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.337489>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.2191887>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.110858>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.0577419>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.9976485>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.9458241>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.8965232>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.8671184>}\n",
            "\n",
            "CPU times: user 2.16 s, sys: 178 ms, total: 2.34 s\n",
            "Wall time: 1.47 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIvigTqaEcu1"
      },
      "source": [
        "A good test of a new model is to see that it can overfit a single batch of input. Try it, the loss should quickly go to zero:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "U-dIWMIBqK7b",
        "outputId": "5babda87-1656-40f8-95ed-3673b91cb054"
      },
      "source": [
        "losses = []\n",
        "for n in range(100):\n",
        "  print('.', end='')\n",
        "  logs = translator.train_step([example_input_batch, example_target_batch])\n",
        "  losses.append(logs['batch_loss'].numpy())\n",
        "\n",
        "print()\n",
        "plt.plot(losses)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "....................................................................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3bf2fa9350>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI02XFjoEt1k"
      },
      "source": [
        "Now that you're confident that the training step is working, build a fresh copy of the model to train from scratch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emgfgh4tAmJt"
      },
      "source": [
        "train_translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpObfY22IddU"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "While there's nothing wrong with writing your own custom training loop, implementing the `Model.train_step` method, as in the previous section, allows you to run `Model.fit` and avoid rewriting all that boiler-plate code. \n",
        "\n",
        "This tutorial only trains for a couple of epochs, so use a `callbacks.Callback` to collect the history of batch losses, for plotting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7m4mtnj80sq"
      },
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10uj9OXRTKvL"
      },
      "source": [
        "# gpu_options = tf.GPUOptions(allow_growth=True)\n",
        "# session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQd_esVVoSf3",
        "outputId": "6770db62-166d-40e6-90c0-6b1335780f29"
      },
      "source": [
        "train_translator.fit(dataset, epochs=10,\n",
        "                     callbacks=[batch_loss])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 317s 200ms/step - batch_loss: 2.0998\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 312s 200ms/step - batch_loss: 1.0244\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 314s 201ms/step - batch_loss: 0.7538\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 310s 199ms/step - batch_loss: 0.6142\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 313s 200ms/step - batch_loss: 0.5202\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 313s 200ms/step - batch_loss: 0.4552\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 313s 200ms/step - batch_loss: 0.4060\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 313s 200ms/step - batch_loss: 0.3707\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 315s 202ms/step - batch_loss: 0.3407\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 316s 202ms/step - batch_loss: 0.3206\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3bf2f567d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38rLdlmtQHCm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "da727886-bf35-44bc-fd3f-2c77cb8a2de0"
      },
      "source": [
        "plt.plot(batch_loss.logs)\n",
        "plt.ylim([0, 3])\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('CE/token')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'CE/token')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0S_O_RzHmfe"
      },
      "source": [
        "The visible jumps in the plot are at the epoch boundaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Translate\n",
        "\n",
        "Now that the model is trained, implement a function to execute the full `text => text` translation.\n",
        "\n",
        "For this the model needs to invert the `text => token IDs` mapping provided by the `output_text_processor`. It also needs to know the IDs for special tokens. This is all implemented in the constructor for the new class. The implementation of the actual translate method will follow.\n",
        "\n",
        "Overall this is similar to the training loop, except that the input to the decoder at each time step is a sample from the decoder's last prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO-CLL1LVBbM"
      },
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self,\n",
        "               encoder, decoder, \n",
        "               input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "        vocabulary=output_text_processor.get_vocabulary())\n",
        "    token_mask_ids = index_from_string(['',\n",
        "                                        '[UNK]',\n",
        "                                        '[START]']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string('[START]')\n",
        "    self.end_token = index_from_string('[END]')"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBQzFZ9uWU79"
      },
      "source": [
        "translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b59PN-UxqYrU"
      },
      "source": [
        "### Convert token IDs to text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-razg3Aso737"
      },
      "source": [
        "The first method to implement is `tokens_to_text` which converts from token IDs to human readable text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IjwKTwtmdFf"
      },
      "source": [
        "def tokens_to_text(self, result_tokens):\n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(result_tokens, ('batch', 't'))\n",
        "  result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "  shape_checker(result_text_tokens, ('batch', 't'))\n",
        "\n",
        "  result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                       axis=1, separator=' ')\n",
        "  shape_checker(result_text, ('batch'))\n",
        "\n",
        "  result_text = tf.strings.strip(result_text)\n",
        "  shape_checker(result_text, ('batch',))\n",
        "  return result_text"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "912aV0K7r90w"
      },
      "source": [
        "Translator.tokens_to_text = tokens_to_text"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krBuAapkqNs9"
      },
      "source": [
        "Input some random token IDs and see what it generates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWCMHdoS32QN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f98b72a8-4383-456b-9159-ee6361ae47a7"
      },
      "source": [
        "example_output_tokens = tf.random.uniform(\n",
        "    shape=[5, 2], minval=0, dtype=tf.int64,\n",
        "    maxval=output_text_processor.vocabulary_size())\n",
        "translator.tokens_to_text(example_output_tokens).numpy()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'equipment overanalyzing', b'supportinformation hassle',\n",
              "       b'claustrophobic tomboy', b'girlishly relationship',\n",
              "       b'blame comforted'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC9De_kAqtaE"
      },
      "source": [
        "### Sample from the decoder's predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5tno-2ksJv6"
      },
      "source": [
        "This function takes the decoder's logit outputs and samples token IDs from that distribution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lfuj3GcdD6e"
      },
      "source": [
        "def sample(self, logits, temperature):\n",
        "  shape_checker = ShapeChecker()\n",
        "  # 't' is usually 1 here.\n",
        "  shape_checker(logits, ('batch', 't', 'vocab'))\n",
        "  shape_checker(self.token_mask, ('vocab',))\n",
        "\n",
        "  token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "  shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n",
        "\n",
        "  # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "  logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "\n",
        "  if temperature == 0.0:\n",
        "    new_tokens = tf.argmax(logits, axis=-1)\n",
        "  else: \n",
        "    logits = tf.squeeze(logits, axis=1)\n",
        "    new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                        num_samples=1)\n",
        "  \n",
        "  shape_checker(new_tokens, ('batch', 't'))\n",
        "\n",
        "  return new_tokens"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DpDnBdBdL9_"
      },
      "source": [
        "Translator.sample = sample"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwdHfGEfsmy5"
      },
      "source": [
        "Test run this function on some random inputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwLT0nxXym80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb0a5e35-b581-4c40-d323-a63572923b0c"
      },
      "source": [
        "example_logits = tf.random.normal([5, 1, output_text_processor.vocabulary_size()])\n",
        "example_output_tokens = translator.sample(example_logits, temperature=1.0)\n",
        "example_output_tokens"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
              "array([[9878],\n",
              "       [9618],\n",
              "       [2381],\n",
              "       [1185],\n",
              "       [ 845]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEWIKFIJ2HWM"
      },
      "source": [
        "### Implement the translation loop\n",
        "\n",
        "Here is a complete implementation of the text to text translation loop.\n",
        "\n",
        "This implementation collects the results into python lists, before using `tf.concat` to join them into tensors.\n",
        "\n",
        "This implementation statically unrolls the graph out to `max_length` iterations.\n",
        "This is okay with eager execution in python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmOvVrZmwAxg"
      },
      "source": [
        "def translate_unrolled(self,\n",
        "                       input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "  batch_size = tf.shape(input_text)[0]\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "  dec_state = enc_state\n",
        "  new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "  result_tokens = []\n",
        "  attention = []\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                             enc_output=enc_output,\n",
        "                             mask=(input_tokens!=0))\n",
        "    \n",
        "    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "    attention.append(dec_result.attention_weights)\n",
        "\n",
        "    new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "    # If a sequence produces an `end_token`, set it `done`\n",
        "    done = done | (new_tokens == self.end_token)\n",
        "    # Once a sequence is done it only produces 0-padding.\n",
        "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "    # Collect the generated tokens\n",
        "    result_tokens.append(new_tokens)\n",
        "\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Convert the list of generates token ids to a list of strings.\n",
        "  result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "  result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "  if return_attention:\n",
        "    attention_stack = tf.concat(attention, axis=1)\n",
        "    return {'text': result_text, 'attention': attention_stack}\n",
        "  else:\n",
        "    return {'text': result_text}\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOmd8Y269MG3"
      },
      "source": [
        "Translator.translate = translate_unrolled"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxYXf3GNKKLS"
      },
      "source": [
        "Run it on a simple input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd2rgyHwVVrv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96fe70d0-7ab3-476d-d06d-d253e07ec5d2"
      },
      "source": [
        "%%time\n",
        "input_text = tf.constant([\n",
        "'Covid 19 onay formu',\n",
        "      'Son zamanlarda yurt dışına gittiniz mi?'\n",
        "      'Hastanın ismi',\n",
        "      'Arabayı dikkatli sür.',\n",
        "      'Sigara içiyorsun'\n",
        "      'aşı oldunuz mu?',\n",
        "      'kusma / ishal',\n",
        "      'Neden tedavi olmak istiyorsun?'\n",
        "      'yurtdışındaydın',\n",
        "      'imza',\n",
        "      'aşağıdaki belirtilerden herhangi birini gösterdiniz mi?',\n",
        "      'hikaye',\n",
        "      'toplantı',\n",
        "      'doğum günü'\n",
        "])\n",
        "\n",
        "result = translator.translate(\n",
        "    input_text = input_text)\n",
        "\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print(result['text'][2].numpy().decode())\n",
        "print(result['text'][3].numpy().decode())\n",
        "print(result['text'][4].numpy().decode())\n",
        "print(result['text'][5].numpy().decode())\n",
        "print()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "covid acknowledgement form\n",
            "have you been abroad .\n",
            "drive cautiously .\n",
            "have you had a complaint ?\n",
            "vomiting person exists .\n",
            "why you want treatment ?\n",
            "\n",
            "CPU times: user 206 ms, sys: 3.7 ms, total: 210 ms\n",
            "Wall time: 205 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-6cFyqeUPQm"
      },
      "source": [
        "If you want to export this model you'll need to wrap this method in a `tf.function`. This basic implementation has a few issues if you try to do that:\n",
        "\n",
        "1. The resulting graphs are very large and take a few seconds to build, save or load.\n",
        "2. You can't break from a statically unrolled loop, so it will always run `max_length` iterations, even if all the outputs are done. But even then it's marginally faster than eager execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JhTZ5hOptO-"
      },
      "source": [
        "f = tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "def tf_translate(self, input_text):\n",
        "  return self.translate(input_text)\n",
        "\n",
        "Translator.tf_translate = tf_translate"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkccvHDvXCa8"
      },
      "source": [
        "Run the `tf.function` once to compile it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NzrixLvVBjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "825f2106-88d1-4773-ad38-8ac16b44615f"
      },
      "source": [
        "%%time\n",
        "result = translator.tf_translate(\n",
        "    input_text = input_text)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 199 ms, sys: 5.5 ms, total: 204 ms\n",
            "Wall time: 200 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USJdu00tVFbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26bdc7ec-0c16-41b3-d552-eaab7638cb14"
      },
      "source": [
        "%%time\n",
        "result = translator.tf_translate(\n",
        "    input_text = input_text)\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print(result['text'][2].numpy().decode())\n",
        "\n",
        "\n",
        "print()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "covid acknowledgement form\n",
            "have you been abroad in the last days ?\n",
            "drive cautiously .\n",
            "\n",
            "CPU times: user 222 ms, sys: 5.91 ms, total: 228 ms\n",
            "Wall time: 223 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbQpyYs13jF_"
      },
      "source": [
        "#@title [Optional] Use a symbolic loop\n",
        "def translate_symbolic(self,\n",
        "                       input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(input_text, ('batch',))\n",
        "\n",
        "  batch_size = tf.shape(input_text)[0]\n",
        "\n",
        "  # Encode the input\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  shape_checker(input_tokens, ('batch', 's'))\n",
        "\n",
        "  enc_output, enc_state = self.encoder(input_tokens)\n",
        "  shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
        "  shape_checker(enc_state, ('batch', 'enc_units'))\n",
        "\n",
        "  # Initialize the decoder\n",
        "  dec_state = enc_state\n",
        "  new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "  shape_checker(new_tokens, ('batch', 't1'))\n",
        "\n",
        "  # Initialize the accumulators\n",
        "  result_tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)\n",
        "  attention = tf.TensorArray(tf.float32, size=1, dynamic_size=True)\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "  shape_checker(done, ('batch', 't1'))\n",
        "\n",
        "  for t in tf.range(max_length):\n",
        "    dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                             enc_output=enc_output,\n",
        "                             mask = (input_tokens!=0))\n",
        "\n",
        "    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "    shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
        "    attention = attention.write(t, dec_result.attention_weights)\n",
        "\n",
        "    new_tokens = self.sample(dec_result.logits, temperature)\n",
        "    shape_checker(dec_result.logits, ('batch', 't1', 'vocab'))\n",
        "    shape_checker(new_tokens, ('batch', 't1'))\n",
        "\n",
        "    # If a sequence produces an `end_token`, set it `done`\n",
        "    done = done | (new_tokens == self.end_token)\n",
        "    # Once a sequence is done it only produces 0-padding.\n",
        "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "    # Collect the generated tokens\n",
        "    result_tokens = result_tokens.write(t, new_tokens)\n",
        "\n",
        "    if tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Convert the list of generates token ids to a list of strings.\n",
        "  result_tokens = result_tokens.stack()\n",
        "  shape_checker(result_tokens, ('t', 'batch', 't0'))\n",
        "  result_tokens = tf.squeeze(result_tokens, -1)\n",
        "  result_tokens = tf.transpose(result_tokens, [1, 0])\n",
        "  shape_checker(result_tokens, ('batch', 't'))\n",
        "\n",
        "  result_text = self.tokens_to_text(result_tokens)\n",
        "  shape_checker(result_text, ('batch',))\n",
        "\n",
        "  if return_attention:\n",
        "    attention_stack = attention.stack()\n",
        "    shape_checker(attention_stack, ('t', 'batch', 't1', 's'))\n",
        "\n",
        "    attention_stack = tf.squeeze(attention_stack, 2)\n",
        "    shape_checker(attention_stack, ('t', 'batch', 's'))\n",
        "\n",
        "    attention_stack = tf.transpose(attention_stack, [1, 0, 2])\n",
        "    shape_checker(attention_stack, ('batch', 't', 's'))\n",
        "\n",
        "    return {'text': result_text, 'attention': attention_stack}\n",
        "  else:\n",
        "    return {'text': result_text}"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngywxv1WYO_O"
      },
      "source": [
        "Translator.translate = translate_symbolic"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lItV7qjEGsYc"
      },
      "source": [
        "The initial implementation used python lists to collect the outputs. This uses `tf.range` as the loop iterator, allowing `tf.autograph` to convert the loop. The biggest change in this implementation is the use of `tf.TensorArray` instead of python `list` to accumulate tensors. `tf.TensorArray` is required to collect a variable number of tensors in graph mode. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ_NznOgZTxC"
      },
      "source": [
        "With eager execution this implementation performs on par with the original:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRh66y-YYeBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "623daaf3-01a6-4ab0-eee8-fe39279599ea"
      },
      "source": [
        "%%time\n",
        "result = translator.translate(\n",
        "    input_text = input_text)\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print(result['text'][2].numpy().decode())\n",
        "print()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "covid acknowledgement form\n",
            "have you been abroad in the last days ?\n",
            "drive cautiously .\n",
            "\n",
            "CPU times: user 251 ms, sys: 4.49 ms, total: 256 ms\n",
            "Wall time: 253 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6B8W4_MZdX0"
      },
      "source": [
        "But when you wrap it in a `tf.function` you'll notice two differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX6EF8KtYh20"
      },
      "source": [
        "@tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "def tf_translate(self, input_text):\n",
        "  return self.translate(input_text)\n",
        "\n",
        "Translator.tf_translate = tf_translate"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S0kQ-bBZswZ"
      },
      "source": [
        "First: Graph creation is much faster (~10x), since it doesn't create `max_iterations` copies of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq8d40RKYoJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd2ad93-050e-4835-c01d-036b7d461b06"
      },
      "source": [
        "%%time\n",
        "result = translator.tf_translate(\n",
        "    input_text = input_text)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.48 s, sys: 35.2 ms, total: 1.52 s\n",
            "Wall time: 1.49 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ABEwtKIZ6eE"
      },
      "source": [
        "Second: The compiled function is much faster on small inputs (5x on this example), because it can break out of the loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5VdCLxPYrpz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378ccfa1-0034-40ca-e8b5-fef5f9cecea4"
      },
      "source": [
        "%%time\n",
        "result = translator.tf_translate(\n",
        "    input_text = input_text)\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print(result['text'][2].numpy().decode())\n",
        "\n",
        "print(result['text'][3].numpy().decode())\n",
        "print(result['text'][4].numpy().decode())\n",
        "print(result['text'][5].numpy().decode())\n",
        "print(result['text'][6].numpy().decode())\n",
        "print(result['text'][7].numpy().decode())\n",
        "print(result['text'][8].numpy().decode())\n",
        "print(result['text'][9].numpy().decode())\n",
        "print(result['text'][10].numpy().decode())\n",
        "\n",
        "print()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "covid acknowledgement form\n",
            "have you been abroad in the last days ?\n",
            "drive cautiously .\n",
            "have you been in drug with a drug confirmed ?\n",
            "vomiting o realistic\n",
            "why you want treatment ?\n",
            "signature\n",
            "did you have any of the following symptoms ?\n",
            "story\n",
            "baffled\n",
            "birth date\n",
            "\n",
            "CPU times: user 61.4 ms, sys: 5.86 ms, total: 67.3 ms\n",
            "Wall time: 51.9 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo5sf4jZaO2l"
      },
      "source": [
        "### Visualize the process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzZzC2cJacTv"
      },
      "source": [
        "The attention weights returned by the `translate` method show where the model was \"looking\" when it generated each output token.\n",
        "\n",
        "So the sum of the attention over the input should return all ones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEd2GljgqQ-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3401d677-821d-45ad-e8b1-174bd544b275"
      },
      "source": [
        "a = result['attention'][0]\n",
        "\n",
        "print(np.sum(a, axis=-1))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.         0.99999994 0.9999999  1.         1.         1.\n",
            " 1.         1.         1.         1.         1.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_HWQHcI2_h5"
      },
      "source": [
        "Here is the attention distribution for the first output step of the first example. Note how the attention is now much more focused than it was for the untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8BHdqQujALu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "ae0af991-b977-4c5d-c915-04a721e19248"
      },
      "source": [
        "_ = plt.bar(range(len(a[0, :])), a[0, :])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB13OG472Z3V"
      },
      "source": [
        "Since there is some rough alignment between the input and output words, you expect the attention to be focused near the diagonal:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXECcNTn2mxN"
      },
      "source": [
        "Here is some code to make a better attention plot:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHBdOf9duumm"
      },
      "source": [
        "Translate a few more sentences and plot them:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA3xI3NzrRJt"
      },
      "source": [
        "The short sentences often work well, but if the input is too long the model literally loses focus and stops providing reasonable predictions. There are two main reasons for this:\n",
        "\n",
        "1. The model was trained with teacher-forcing feeding the correct token at each step, regardless of the model's predictions. The model could be made more robust if it were sometimes fed its own predictions.\n",
        "2. The model only has access to its previous output through the RNN state. If the RNN state gets corrupted, there's no way for the model to recover. [Transformers](transformer.ipynb) solve this by using self-attention in the encoder and decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMA9Pp71nzH9"
      },
      "source": [
        "## Export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rLMNOmsKoXe"
      },
      "source": [
        "Once you have a model you're satisfied with you might want to export it as a `tf.saved_model` for use outside of this python program that created it.\n",
        "\n",
        "Since the model is a subclass of `tf.Module` (through `keras.Model`), and all the functionality for export is compiled in a `tf.function` the model should export cleanly with `tf.saved_model.save`:  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP2dNtEXJPEL"
      },
      "source": [
        "Now that the function has been traced it can be exported using `saved_model.save`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyvxT5V0_X5B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b4449eb-3069-4b45-9628-87963b82be4f"
      },
      "source": [
        "tf.saved_model.save(translator, 'Turkish',\n",
        "                    signatures={'serving_default': translator.tf_translate})"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as encoder_2_layer_call_fn, encoder_2_layer_call_and_return_conditional_losses, decoder_2_layer_call_fn, decoder_2_layer_call_and_return_conditional_losses, embedding_4_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: Turkish/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: Turkish/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdTHXErbHEjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ef5695-86f8-4e90-bda4-875cff6ab118"
      },
      "source": [
        "!zip -r /content/file1.zip /content/Turkish"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/Turkish/ (stored 0%)\n",
            "  adding: content/Turkish/variables/ (stored 0%)\n",
            "  adding: content/Turkish/variables/variables.index (deflated 55%)\n",
            "  adding: content/Turkish/variables/variables.data-00000-of-00001 (deflated 8%)\n",
            "  adding: content/Turkish/saved_model.pb (deflated 89%)\n",
            "  adding: content/Turkish/assets/ (stored 0%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mVRSbYsHSWd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "b13baf63-0b32-44fa-d173-0dc4cee19b13"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/file1.zip\")"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_7c50520b-8d28-4bf9-8252-9d905d794b66\", \"file1.zip\", 102026647)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMgDv9HJK5xQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97bde8a-56fb-4d06-f388-0cb3c0305687"
      },
      "source": [
        "%%time\n",
        "three_input_text = tf.constant([\n",
        "    'covid 19 onay formu', # \"It's really cold here.\"\n",
        "    'lütfen gitme, biraz daha kal', # \"This is my life.\"\"\n",
        "    'doktorun adı', # \"This is my life.\"\"\n",
        "    'ne zaman aşı oldunuz', # \"This is my life.\"\"\n",
        "    'kusma / ishal', # \"This is my life.\"\"\n",
        "    'tarafımız sorumluluk kabul etmeyecektir', # \"This is my life.\"\"\n",
        "    'neden tedavi görmek istiyorsunuz', # \"This is my life.\"\"\n",
        "    'sigara kullanıyor musunuz', # \"This is my life.\"\"\n",
        "    'yurt dışında bulundunuz mu', # \"This is my life.\"\"\n",
        "    'aşağıdaki semptomlardan birni gösterdiniz mi', # \"This is my life.\"\"\n",
        "    'okudum anlıyorum ve onaylıyorum', # \"This is my life.\"\"\n",
        "    'alkol kullanmayı bırakmalısın', # \"This is my life.\"\"\n",
        "    'aşağıdaki formu doldurunuz', # \"This is my life.\"\"\n",
        "    'emin misiniz', # \"This is my life.\"\"\n",
        "    'sorumlululk tamamen bana aittir', # \"This is my life.\"\"\n",
        "\n",
        "])\n",
        "\n",
        "result = translator.tf_translate(three_input_text)\n",
        "\n",
        "for tr in result['text']:\n",
        "  print(tr.numpy().decode())\n",
        "\n",
        "print()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "covid acknowledgement form\n",
            "come on , please again .\n",
            "birth name\n",
            "when did you eat tobacco ?\n",
            "vomiting person lower suv\n",
            "a liability release discharges the following\n",
            "why i seeking treatment ?\n",
            "do you smoke ?\n",
            "have you been abroad ?\n",
            "are you in any of the following symptoms ?\n",
            "i study and i read .\n",
            "you must sell alcohol .\n",
            "signature\n",
            "are you going to be ?\n",
            "i have it returned ok .\n",
            "\n",
            "CPU times: user 68.4 ms, sys: 13.9 ms, total: 82.3 ms\n",
            "Wall time: 59.7 ms\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}